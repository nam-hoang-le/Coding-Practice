{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"góp gió gặt bão\", \n",
    "    \"có làm mới có ăn\", \n",
    "    \"đất lành chim đậu\", \n",
    "    \"ăn cháo đá bát\", \n",
    "    \"gậy ông đập lưng ông\", \n",
    "    \"qua cầu rút ván\"\n",
    "]\n",
    "\n",
    "# length document\n",
    "n_doc = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: positive - 0: negative\n",
    "labels = [1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict stores categories\n",
    "categories_label = {\n",
    "    \"positive\": 1, \n",
    "    \"negative\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_categories(labels): \n",
    "    key_list = list(categories_label.keys())\n",
    "    val_list = list(categories_label.values())\n",
    "    position = [val_list.index(label) for label in labels]\n",
    "    return np.array(key_list)[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(corpus)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['góp gió gặt bão', 'có làm mới có ăn', 'đất lành chim đậu',\n",
       "        'ăn cháo đá bát', 'gậy ông đập lưng ông', 'qua cầu rút ván'],\n",
       "       dtype='<U20'),\n",
       " array([1, 1, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tf-idf function\n",
    "def calculate_tf_idf(X_vectorized, n_doc):\n",
    "    \n",
    "    # Term Frequency (TF) calculation\n",
    "    tf = np.log(X_vectorized + 1)  # Applying log transformation to term frequencies\n",
    "    \n",
    "    # Document Frequency (DF) calculation\n",
    "    df = np.sum(X_vectorized, axis=0)  # Summing up the occurrence of each term across all documents\n",
    "    \n",
    "    # Inverse Document Frequency (IDF) calculation\n",
    "    idf = np.log((n_doc + 1) / (df + 1)) + 1  # Adding 1 to avoid division by zero, and applying log transformation\n",
    "    \n",
    "    # TF-IDF calculation\n",
    "    tf_idf = tf * idf  # Multiplying TF with IDF\n",
    "    \n",
    "    return idf, tf, tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate norm function\n",
    "def compute_norm(tf_idf_vec): \n",
    "    # Calculate the Euclidean norm of each TF-IDF vector\n",
    "    norm = np.linalg.norm(tf_idf_vec, axis=1)\n",
    "\n",
    "    # Get the number of documents\n",
    "    n_doc = tf_idf_vec.shape[0]\n",
    "\n",
    "    # Normalize each TF-IDF vector by its corresponding norm\n",
    "    normalized_tf_idf_vec = np.empty_like(tf_idf_vec)\n",
    "    for idx in range(n_doc):\n",
    "        normalized_tf_idf_vec[idx] = tf_idf_vec[idx] / norm[idx]\n",
    "\n",
    "    return normalized_tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  ['bát' 'bão' 'chim' 'cháo' 'có' 'cầu' 'gió' 'góp' 'gậy' 'gặt' 'làm' 'lành'\n",
      " 'lưng' 'mới' 'qua' 'rút' 'ván' 'ông' 'ăn' 'đá' 'đất' 'đập' 'đậu']\n"
     ]
    }
   ],
   "source": [
    "# change to vector \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit to normalization\n",
    "X_vectorized = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# print it out\n",
    "print(\"Vocab: \", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.25276297, 2.25276297, 2.25276297, 2.25276297, 1.84729786,\n",
       "        2.25276297, 2.25276297, 2.25276297, 2.25276297, 2.25276297,\n",
       "        2.25276297, 2.25276297, 2.25276297, 2.25276297, 2.25276297,\n",
       "        2.25276297, 2.25276297, 1.84729786, 1.84729786, 2.25276297,\n",
       "        2.25276297, 2.25276297, 2.25276297]),\n",
       " array([[0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.69314718, 0.69314718, 0.        , 0.69314718,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 1.09861229,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69314718, 0.        , 0.        , 0.69314718, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.69314718, 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.69314718, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69314718, 0.        , 0.69314718],\n",
       "        [0.69314718, 0.        , 0.        , 0.69314718, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.69314718, 0.69314718,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.69314718, 0.        ,\n",
       "         0.        , 0.        , 0.69314718, 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.09861229, 0.        , 0.        ,\n",
       "         0.        , 0.69314718, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69314718, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
       "         0.69314718, 0.69314718, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]),\n",
       " array([[0.        , 1.5614963 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 1.5614963 , 1.5614963 , 0.        , 1.5614963 ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 2.02946413,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         1.5614963 , 0.        , 0.        , 1.5614963 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.2804493 , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.5614963 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 1.5614963 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         1.5614963 , 0.        , 1.5614963 ],\n",
       "        [1.5614963 , 0.        , 0.        , 1.5614963 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.2804493 , 1.5614963 ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.5614963 , 0.        ,\n",
       "         0.        , 0.        , 1.5614963 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 2.02946413, 0.        , 0.        ,\n",
       "         0.        , 1.5614963 , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         1.5614963 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 1.5614963 ,\n",
       "         1.5614963 , 1.5614963 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_idf, X_tf, X_tf_idf = calculate_tf_idf(X_vectorized, n_doc)\n",
    "X_idf, X_tf, X_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5       , 0.5       , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.62232376,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.47882405, 0.        , 0.        , 0.47882405, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.39264257, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.5       ],\n",
       "       [0.52182349, 0.        , 0.        , 0.52182349, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.42790272, 0.52182349,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.46179653, 0.        ,\n",
       "        0.        , 0.        , 0.46179653, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.60019322, 0.        , 0.        ,\n",
       "        0.        , 0.46179653, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change to L2 norm\n",
    "compute_norm(X_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# get model \n",
    "knn_cls = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_cls.fit(X_tf_idf, y)\n",
    "preds = knn_cls.predict(X_tf_idf)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change to vector\n",
    "test_text = np.array([\"không làm cạp đất mà ăn\"])\n",
    "test_vec = vectorizer.transform(test_text).toarray()\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       , 1.5614963, 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        1.2804493, 0.       , 1.5614963, 0.       , 0.       ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tf = np.log(test_vec + 1)\n",
    "test_tf_idf = test_tf * X_idf\n",
    "test_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.61171251, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.50161301, 0.        ,\n",
       "        0.61171251, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_norm(test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "# change to label\n",
    "pred = knn_cls.predict(test_tf_idf)\n",
    "\n",
    "print(label_to_categories(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
